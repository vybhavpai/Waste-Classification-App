{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\nimport random\nimport numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport cv2\n\nDATADIR = '/kaggle/input/waste-classification-data/dataset/DATASET/TRAIN'\nCATEGORIES = ['O', 'R']\n\nfor category in CATEGORIES:\n    path = os.path.join(DATADIR, category) # path to O or R dir\n    for img in os.listdir(path):\n        img_array = cv2.imread(os.path.join(path, img))\n        plt.imshow(img_array, cmap='gray')\n        # plt.show()\n        break\n    break\n\nprint(img_array.shape)\n\nIMG_SIZE = 100                             \nnew_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE))\nplt.imshow(new_array)\n# plt.show()\n\ntraining_data = []\n\ndef create_training_data():\n    for category in CATEGORIES:\n        path = os.path.join(DATADIR, category) # path to O or R dir\n        class_num = CATEGORIES.index(category)\n        for img in os.listdir(path):\n            try:\n                img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_COLOR)\n                new_array = cv2.resize(img_array, (IMG_SIZE, IMG_SIZE), 3)\n                training_data.append([new_array, class_num])\n            except Exception as e:\n                pass\ncreate_training_data()\n\nprint(len(training_data))\n\nrandom.shuffle(training_data)\n\nX = []\ny = []\n\nfor features, label in training_data:\n    X.append(features)\n    y.append(label)\n\nX = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 3) # 1: because it is grayscale\n\npickle_out = open('X.pickle', 'wb')\npickle.dump(X, pickle_out)\npickle_out.close()\n\npickle_out = open('y.pickle', 'wb')\npickle.dump(y, pickle_out)\npickle_out.close()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation, Flatten\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D\nimport tensorflow as tf\nimport pickle\nimport time\nimport numpy as np\n\n\npickle_in = open(\"X.pickle\", \"rb\")\nX = pickle.load(pickle_in)\n\npickle_in = open(\"y.pickle\", \"rb\")\ny = pickle.load(pickle_in)\n\nX = X/255.0\n\nX = np.array(X).reshape(-1, IMG_SIZE, IMG_SIZE, 3)\n\ny = np.array(y)\n\ndense_layers = [0]\nlayer_sizes = [128]\nconv_layers = [3]\n\nfor dense_layer in dense_layers:\n    for layer_size in layer_sizes:\n        for conv_layer in conv_layers:\n            NAME = \"{}-conv-{}-nodes-{}-dense-{}\".format(\n                conv_layer, layer_size, dense_layer, int(time.time()))\n            print(NAME)\n\n            model = Sequential()\n\n            model.add(Conv2D(layer_size, (3, 3), input_shape=X.shape[1:]))\n            model.add(Activation('relu'))\n            model.add(MaxPooling2D(pool_size=(2, 2)))\n\n            for l in range(conv_layer-1):\n                model.add(Conv2D(layer_size, (3, 3)))\n                model.add(Activation('relu'))\n                model.add(MaxPooling2D(pool_size=(2, 2)))\n\n            model.add(Flatten())\n\n            for _ in range(dense_layer):\n                model.add(Dense(layer_size))\n                model.add(Activation('relu'))\n                model.add(Dropout(0.5))\n\n            model.add(Dense(1))\n            model.add(Activation('sigmoid'))\n#             print('came here')\n            opt = tf.keras.optimizers.Adam(learning_rate=0.001)\n\n            model.compile(loss='binary_crossentropy',\n                            optimizer=opt,\n                            metrics=['accuracy'])\n            print('came here')\n            model.fit(X, y,\n                        batch_size=32,\n                        epochs=10,\n                        validation_split=0.2)\n\nmodel.save('128x3-CNN.model')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import Conv2D, Activation,Dropout\nfrom keras.models import Model,load_model\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers.pooling import MaxPooling2D\nfrom keras.layers.core import Flatten, Dense\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau\nfrom keras.engine.topology import Layer\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras import backend as K\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.datasets import load_files\nimport itertools\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport cv2\nimport matplotlib.pyplot as plt\nimport itertools\n\ntrain_dir = '/kaggle/input/waste-classification-data/dataset/DATASET/TRAIN'\ntest_dir = '/kaggle/input/waste-classification-data/dataset/DATASET/TEST'\n\ndef load_dataset(path):\n    data = load_files(path) #load all files from the path\n    files = np.array(data['filenames']) #get the file  \n    targets = np.array(data['target'])#get the the classification labels as integer index\n    target_labels = np.array(data['target_names'])#get the the classification labels \n    return files,targets,target_labels\n    \nx_train, y_train,target_labels = load_dataset(train_dir)\nx_test, y_test,_ = load_dataset(test_dir)\n\nprint('Training set size : ' , x_train.shape[0])\nprint('Testing set size : ', x_test.shape[0])\n\n# x_train,x_validate,y_train,y_validate = train_test_split(x_train,y_train,test_size = 0.2,random_state = 1)\n\nprint (\"x_train shape: \" + str(x_train.shape))\nprint (\"y_train shape: \" + str(y_train.shape))\n# print (\"x_validate shape: \" + str(x_validate.shape))\n# print (\"y_validate shape: \" + str(y_validate.shape))\nprint (\"x_test shape: \" + str(x_test.shape))\nprint (\"y_test shape: \" + str(y_test.shape))\n\ndef convert_image_to_array(files):\n    width, height, channels = 100, 100, 3\n    images_as_array = np.empty((files.shape[0], width, height, channels), dtype=np.uint8) #define train and test data shape\n    for idx,file in enumerate(files):\n        img = cv2.imread(file) \n        res = cv2.resize(img, dsize=(width, height), interpolation=cv2.INTER_CUBIC) #As images have different size, resizing all images to have same shape of image array\n        images_as_array[idx] = res\n    return images_as_array\n\nx_train = np.array(convert_image_to_array(x_train))\nprint('Training set shape : ',x_train.shape)\n\n# x_valid = np.array(convert_image_to_array(x_validate))\n# print('Validation set shape : ',x_valid.shape)\n\nx_test = np.array(convert_image_to_array(x_test))\nprint('Test set shape : ',x_test.shape)\n\nx_train = x_train.astype('float32')/255\n# x_valid = x_valid.astype('float32')/255\nx_test = x_test.astype('float32')/255\ny_train = y_train.reshape(y_train.shape[0],1)\ny_test = y_test.reshape(y_test.shape[0],1)\n# y_validate = y_validate.reshape(y_validate.shape[0],1)\n\nplt.figure(figsize=(20,20))\nclasses = ['R','O']\nfor i in range(1,26):\n    index = np.random.randint(x_test.shape[0])\n    plt.subplot(5, 5, i)\n    plt.imshow(np.squeeze(x_test[index]), cmap='cool')\n    plt.title(classes[int(y_test[index])])\n    plt.tight_layout()\nplt.show()                                                                                  ","execution_count":null,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"},{"output_type":"stream","text":"Training set size :  22564\nTesting set size :  2513\nx_train shape: (22564,)\ny_train shape: (22564,)\nx_test shape: (2513,)\ny_test shape: (2513,)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pickle\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n\nmodel = tf.keras.models.load_model(\"128x3-CNN.model\")\n\nscore_train = model.evaluate(x_train, y_train, verbose=0)\nprint('Train Loss: ', score_train[0])\nprint('Train Accuracy: ', score_train[1])\n\nscore = model.evaluate(x_test,y_test,verbose=0)\nprint('Test Loss :',score[0])\nprint('Test Accuracy :',score[1])\n\n#get the predictions for the test data\npredicted_classes = model.predict_classes(x_test)\n\nconfusion_mtx = confusion_matrix(y_test, predicted_classes) \n\nplt.imshow(confusion_mtx, interpolation='nearest', cmap=plt.cm.Blues)\nplt.title('confusion_matrix')\nplt.colorbar()\ntick_marks = np.arange(2)\nplt.xticks(tick_marks, ['R','O'], rotation=90)\nplt.yticks(tick_marks, ['R','O'])\n#Following is to mention the predicated numbers in the plot and highligh the numbers the most predicted number for particular label\nthresh = confusion_mtx.max() / 2.\nfor i, j in itertools.product(range(confusion_mtx.shape[0]), range(confusion_mtx.shape[1])):\n    plt.text(j, i, confusion_mtx[i, j],\n    horizontalalignment=\"center\",\n    color=\"white\" if confusion_mtx[i, j] > thresh else \"black\")\n\nplt.tight_layout()\nplt.ylabel('True label')\nplt.xlabel('Predicted label')","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}